\documentclass[t]{beamer}
\usepackage[T1]{fontenc}
\usepackage{libertinus}
\usepackage[scaled=0.85]{DejaVuSansMono}
\usepackage{hyperref}
\hypersetup{
colorlinks=true,
linkcolor=blue,
linkbordercolor=blue,
urlcolor=blue,
urlbordercolor=blue,
}
\usepackage{svg}
\usepackage{wrapfig}
\usepackage[outputdir=build]{minted}
\usepackage{verbatim}
\usepackage{soul}
\usepackage{upquote}

\usetheme{Boadilla}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{enumerate items}[circle]

\makeatother
\setbeamertemplate{footline}{%
\hbox{%
\begin{beamercolorbox}[wd=1.0\paperwidth,ht=2.25ex,dp=1ex]{title in head/foot}%
\hspace*{1em}%
\usebeamerfont{title in head/foot}\NoHyper\insertshorttitle\endNoHyper%
\hfill%
\insertframenumber{} / \inserttotalframenumber
\hspace*{1em}%
\end{beamercolorbox}}%
}
\makeatletter

\setbeameroption{show notes on second screen=right}
%\setbeameroption{hide notes}
\setbeamertemplate{note page}{\insertnote\par}

\title{A ``third way'' of compiling polymorphism}
\author{Isaac Elliott}
\date{13 May, 2025}

\begin{document}


\frame{\titlepage}


\begin{frame}[fragile]
\frametitle{Compiling a function}

\begin{columns}
\begin{column}{0.48\textwidth}
\begin{block}{A simple function}
\footnotesize
\begin{minted}{c}
int64_t add(int64_t x, int64_t y) {
    return x + y;
}
\end{minted}
\end{block}
\end{column}

\begin{column}{0.48\textwidth}
\begin{block}{A simple function call}
\begin{minted}{c}
int64_t z = add(42, 99);
\end{minted}
\end{block}
\end{column}
\end{columns}

\begin{columns}

\pause

\begin{column}{0.48\textwidth}
\begin{block}{Plausible machine code for \texttt{add}}
\begin{minted}{nasm}
add:
mov rax, [rsp + 8]
mov rbx, [rsp + 16]
add rax, rbx
mov [rsp + 24], rax
ret
\end{minted}
\end{block}
\end{column}

\begin{column}{0.48\textwidth}
\begin{block}{Plausible machine code for \textit{calling} \texttt{add}}
\begin{minted}{nasm}
push QWORD 42
push QWORD 99
call add
mov [rbp], rax
add rsp, 16
\end{minted}
\end{block}
\end{column}
\end{columns}

\note{
Let's start with some code generation fundamentals.
\\ \\
Here's a simple function that does addition.
}
\note<2->{
\\ \\
Here's some plausible machine code for the function. Let's step through it so you can
get a feel for what it does.
}
\end{frame}


\begin{frame}[fragile]
\frametitle{Compiling a function}

\begin{columns}
\begin{column}{0.48\textwidth}
\begin{block}{A simple function}
\footnotesize
\begin{minted}{c}
int64_t add(int64_t x, int64_t y) {
    return x + y;
}
\end{minted}
\end{block}
\end{column}

\begin{column}{0.48\textwidth}
\begin{block}{A simple function call}
\begin{minted}{c}
int64_t z = add(42, 99);
\end{minted}
\end{block}
\end{column}
\end{columns}

\begin{block}{Program state (\only<1-3>{calling \texttt{add}}\only<4-8>{running \texttt{add}}\only<9->{returned from \texttt{add}})}
\begin{columns}
\begin{column}{0.48\textwidth}
\begin{tabular}{c l}
& Instruction \\
\hline
\only<1-3,9->{
\only<1>{$\rightarrow$} & \texttt{push QWORD 42} \\
\only<2>{$\rightarrow$} & \texttt{push QWORD 99} \\
\only<3>{$\rightarrow$} & \texttt{call add} \\
\only<9>{$\rightarrow$} & \texttt{add rsp, 16} \\
\only<10>{$\rightarrow$ & }
}
\only<4-8>{
\only<4>{$\rightarrow$} & \texttt{mov rax, [rsp + 8]} \\
\only<5>{$\rightarrow$} & \texttt{mov rbx, [rsp + 16]} \\
\only<6>{$\rightarrow$} & \texttt{add rax, rbx} \\
\only<7>{$\rightarrow$} & \texttt{mov [rsp + 24], rax} \\
\only<8>{$\rightarrow$} & \texttt{ret}
}
\end{tabular}
\end{column}

\begin{column}{0.48\textwidth}
\begin{tabular}{l l}
Address & Data \\
\hline
\texttt{0x0} & free \\
\texttt{...} & free \\
\only<4-8>{\texttt{rbp - 32\only<4-8>{ (= rsp)}} & return addr \\}%
\only<3-9>{\texttt{rbp - 24\only<3,9>{ (= rsp)}} & \texttt{99} \\}%
\only<2-9>{\texttt{rbp - 16\only<2>{ (= rsp)}} & \texttt{42} \\}%
\texttt{rbp - 8\only<1,10>{ (= rsp)}} & \only<1-7>{space for \texttt{z}}\only<8->{\texttt{141}} \\
\texttt{rbp} & ?
\end{tabular}
\end{column}
\end{columns}
\end{block}

\note<1>{
On the left we have the program counter. The arrow points to the instruction that
will be executed next.
\\ \\
On the right is the stack, which is a primitive data struture provided to the program
by the operating system. It's just a slab of memory, plus a register \texttt{rsp} that
holds the address of the top of the stack.
\\ \\
By convention, more recent stack entries are stored at lower addresses. The maximum
possible stack location isn't usually \texttt{0x0}; there are usually other parts of
the program that live at those low address. But I thought that putting \texttt{0x0} as
the ``ceiling'' of the stack was very illustrative.
\\ \\
The \texttt{rbp} register holds the ``frame pointer''. Every function call gets to own
part of the stack, and promises not to mess with the portion of the stack owned by its
caller. The frame pointer tells us where our caller's stack ends and ours begins.
Local variables are addressed relative to the frame pointer.
\\ \\
So you can see that before we call \texttt{add} we've allocated memory for its return
value.
}
\note<2>{
We just pushed the 8 byte value \texttt{42} onto the stack.
}
\note<3>{
We just pushed the 8 byte value \texttt{99} onto the stack.
}
\note<4>{
We've called \texttt{add}, which pushes the return address onto the stack and then jumps
to the function's address.
}
\note<5>{
\texttt{99} was loaded into \texttt{rax}
}
\note<6>{
\texttt{42} was loaded into \texttt{rbx}
}
\note<7>{
\texttt{rax} was set to $99 + 42 = 141$
}
\note<8>{
The space for \texttt{z} was filled with that value
}
\note<9>{
We returned from \texttt{add}, which popped the return address from the top of the stack
and then jumped to it.
}
\note<10>{
Two 8-byte values were ``popped'' from the stack. They're technically still there (because
we didn't overwrite them) but for all intents and purposes they're garbage.
}
\end{frame}


\begin{frame}[fragile]
\frametitle{Compiling a function}

\begin{columns}
\begin{column}{0.48\textwidth}
\begin{block}{A simple function}
\footnotesize
\begin{minted}{c}
int64_t add(int64_t x, int64_t y) {
    return x + y;
}
\end{minted}
\end{block}
\end{column}

\begin{column}{0.48\textwidth}
\begin{block}{A simple function call}
\begin{minted}{c}
int64_t z = add(42, 99);
\end{minted}
\end{block}
\end{column}
\end{columns}

\begin{columns}

\begin{column}{0.48\textwidth}
\begin{block}{Plausible machine code for \texttt{add}}
\begin{minted}{nasm}
add:
mov rax, [rsp + 8]
mov rbx, [rsp + 16]
add rax, rbx
mov [rsp + 24], rax
ret
\end{minted}
\end{block}
\end{column}

\begin{column}{0.48\textwidth}
\begin{block}{Plausible machine code for \textit{calling} \texttt{add}}
\begin{minted}{nasm}
push QWORD 42
push QWORD 99
call add
mov [rbp], rax
add rsp, 16
\end{minted}
\end{block}
\end{column}
\end{columns}

\note{
Production compilers avoid passing data via memory where possible, preferring registers,
because accessing memory can be hundreds of times slower.
\\ \\
My example uses memory instead of registers to make \textit{size requirements} very explicit.
At some point, every type must have a known size because it's possible that a value of that type will end up in memory.
In this example values are exclusively placed on the stack, but programs may also store values in the heap (another canonical memory region that I won't explain here).
\\ \\
There is no way to get around size requirements, even when a compiler uses registers effectively.
Registers have fixed sizes, so the compiler needs to know the size of a value to determine which, if any, register can hold the value.
}
\end{frame}


\begin{frame}[fragile]
\frametitle{Compiling a polymorphic function}

\begin{columns}

\begin{column}{0.48\textwidth}
\begin{block}{A simple polymorphic function (C++)}
\begin{minted}{c++}
template <class T>
T id(T x) { return x; }
\end{minted}
\end{block}

What is the size of \texttt{T}?
\end{column}

\begin{column}{0.48\textwidth}
\begin{block}{A simple polymorphic function (Haskell)}
\begin{minted}{haskell}
id :: a -> a
id x = x
\end{minted}
\end{block}

What is the size of \texttt{a}?
\end{column}

\end{columns}

\note{
Polymorphic functions (also known as generics) are defined over \textit{all} types.
\\ \\
In the examples to the left, the names \texttt{T} and \texttt{a} are known as type \textit{variables}.
Since a type variable stands for any possible type, it has no definitive size.
\\ \\
I just said that every type must have a known size.
So how to you compile a function that involves types with \textit{unknown} sizes?
This sounds like a contradiction.
\\ \\
I've chosen C++ and Haskell for my code examples because these languages have very different solutions to this problem.
Let's look at C++ first because C++ comes before Haskell in the dictionary.
}
\end{frame}


\begin{frame}[fragile]
\frametitle{Compiling a polymorphic function (monomorphisation)}

\begin{columns}
\begin{column}{0.48\textwidth}
\begin{block}{A simple polymorphic function (C++)}
\begin{minted}{c++}
template <class T>
T id(T x) { return x; }
\end{minted}
\end{block}
\end{column}
\end{columns}

\begin{columns}

\pause

\begin{column}{0.32\textwidth}
\begin{block}{}
\footnotesize
\begin{minted}{c++}
bool x = id(true);
/*
bool id(bool x) {
    return x;
}
*/
\end{minted}
\end{block}
\end{column}

\pause

\begin{column}{0.32\textwidth}
\begin{block}{}
\footnotesize
\begin{minted}{c++}
int32_t x = id(20);
/*
int32_t id(int32_t x) {
    return x;
}
*/
\end{minted}
\end{block}
\end{column}

\pause

\begin{column}{0.32\textwidth}
\begin{block}{}
\footnotesize
\begin{minted}{c++}
string x = id("hello");
/*
string id(string x) {
    return x;
}
*/
\end{minted}
\end{block}
\end{column}
\end{columns}

\note{
C++ compiles polymorphic functions by \textit{not actually compiling polymorphic functions}.
\\ \\
The definition of a polymorphic function doesn't result in any code generation.
Instead, specialised copies of the polymorphic function are generated whenever it's called with a concrete type.
This process is called \textit{monomorphisation}.
}
\end{frame}


\begin{frame}[fragile]
\frametitle{Pros and cons of monomorphisation}

\begin{table}
\centering
\begin{tabular}{l l}
\centering
Pros & Cons \\
\hline
\visible<2->{Generates efficient code} & \visible<3->{Generates a lot of code} \\
& \visible<4->{No separate compilation} \\
& \visible<5->{Complicates advanced type systems}
\end{tabular}
\end{table}

\note<2>{
The main advantage of this approach is that it generates the same code that you'd write if you used C and copy-pasted a lot of code.
Each monomorphic version of the polymorphic function performs as well as if you'd written is by hand.
This is what's known as a ``zero-cost abstraction''.
\\ \\
Of course, monomorphisation \textit{does} have costs, just not at the level of individual monomorphic functions.
}
\note<3>{
If you use a function like \texttt{id} with 5 different concrete types, then your final program contains 5 slightly different versions of the same function.
Introducting type variables to a function multiplies this effect.
Large programs that use polymorphism liberally tend to exhibit \textit{code bloat} when compiled using monomorphisation.
}
\note<4>{
A related issue is that modules containing polymorphic functions can't be separately compiled.
Separate compilation reduces code bloat by placing a function's code in a single object file that can be referenced by other parts of the program.
Separate compilation also speeds up the compilation process because code for a function definition is not repeatedly generated.
}
\note<5>{
The final drawback I want to mention is how monomorphisation interacts with advanced type system features like higher-kinded types, existential types, generalised algebraic datatypes (GADTs), and dependent types.
My impression as a compiler implementor is that it makes implementing some of these features more difficult, and for other features it's just the wrong approach.
}
\note<6>{
While I've listed few pros relative to the number of cons, I often find that runtime efficiency is worth the trade.
}
\end{frame}


\begin{frame}[fragile]
\frametitle{Compiling a polymorphic function (uniform representation)}

\begin{columns}
\begin{column}{0.40\textwidth}
\begin{block}{A simple polymorphic function (Haskell)}
\begin{minted}{haskell}
id :: a -> a
id x = x
\end{minted}
\end{block}
\end{column}
\end{columns}
\begin{columns}

\begin{column}{0.48\textwidth}
\begin{block}{C equivalent}
\begin{minted}{c}
void* id(void* x) {
    return x;
}
\end{minted}
\end{block}
\end{column}
\end{columns}

\note{
In Haskell, polymorphic functions actually are compiled.
The requirement that ``every type is has a known size'' (including type variables) is satisfied by \textit{making every type (including type variables) have the same size}.
Haskell values are boxed by default, which means they're represented by pointers to heap-allocated memory.
\\ \\
This approach is called \textit{uniform representation}.
}
\end{frame}


\begin{frame}[fragile]
\frametitle{Pros and cons of uniform representation}

\begin{table}
\centering
\begin{tabular}{l l}
Pros & Cons \\
\hline
\visible<2->{Generates compact code} & \visible<5->{Generates allocation-heavy (slow) code} \\
\visible<3->{Allows separate compilation} & \\
\visible<4->{Simplifies advanced type systems} & \\
\end{tabular}
\end{table}

\vspace{0.5cm}

\only<6->{Why are (heap) allocations slow?}
\begin{itemize}
\item<7-> Reduced data locality
\item<8-> Increased computational overhead
\end{itemize}

\note<2>{
Polymorphic functions are compiled once, reducing code size.
}
\note<3>{
This allows modules to be compiled once and reused throughout the program.
}
\note<4>{
Code generation for advanced type system features is no more difficult than the fundamental features.
Functions take and return pointers, from humble booleans to complex dependent types.
}
\note<5>{
The biggest drawback of the uniform representation approach is its reliance on memory.
As I said earlier, accessing memory can be hundreds of times slower than accessing registers.
Overuse of memory and underuse of registers is a potentially massive handicap.
}
\note<7>{
Modern CPUs have layers of caches to reduce the performance gap between registers and memory.
A cache hit is slower than reading from a register but still much faster than accessing memory.
Instead of being read byte-at-a-time, larger chunks of memory (64B at the time of writing) are loaded and cached,
under the assumption that nearby bytes will soon be needed.
When a program is structured such that reading one byte increases the probability of soon reading an adjacent byte,
the program is said to have high \textit{data locality}.
Programs that use a lot of heap allocation tend to have unpredictable data locality.
}
\note<8>{
Another problem is that heap allocation has a computational cost.
The structure of the heap depends on the memory allocation system used by the program.
The allocator has to track which heap regions are free or in use, so that it can reserve an appropriate region when a new allocation is requested.
This bookkeeping work adds up.
}
\note<9>{
Languages that use uniform representation typically rely on clever and complex allocation systems (often called ``garbage collectors'') to reduce these costs.
Garbage collectors can amortise the cost of many allocations over longer programs and improve data locality for common allocation patterns.
\\ \\
But because these allocation systems are general purpose, they're far from optimal for any \textit{specific} function or program.
In this respect, polymorphism via uniform representation is not a ``zero-cost abstraction''.
}
\end{frame}


\begin{frame}[fragile]
\frametitle{A ``third way'' for compiling a polymorphic function}

Goals:
\begin{itemize}
\item Stack allocation by default
\item Allow separate compilation
\item Enable advanced type system features (higher-kinded types, GADTs, dependent types)
\end{itemize}

\note{
Some of the tradeoffs described feel weird.
For example: why should the use of polymorphic functions force me to choose between stack-allocation by default and separate compilation?
For a while I've wondered if there are any other reasonable approaches to compiling polymorphism, and now I've got one that's interesting.
}
\end{frame}


\begin{frame}[fragile]
\frametitle{Compiling a polymorphic function (type passing)}

\begin{columns}
\begin{column}{0.48\textwidth}
\begin{block}{A simple polymorphic function (Haskell-like language)}
\begin{minted}{haskell}
id :: forall a. a -> a
id x = x
\end{minted}
\end{block}
\end{column}
\begin{column}{0.48\textwidth}
\begin{block}{A simple polymorphic function call (Haskell-like language)}
\begin{minted}{haskell}
let x = id @Int64 42
\end{minted}
\end{block}
\end{column}
\end{columns}

\begin{block}{C equivalent (definition)}<only@1,3>
\begin{minted}{c}
void id(void* result, const Type* a, const void* x) {
    // result <- x
    a->move(result, x);
}
\end{minted}
\end{block}

\begin{block}{C equivalent (call)}<only@1,3>
\begin{minted}{c}
int64_t arg = 42;
int64_t x;
id(&x, &Type_Int64, &arg);
\end{minted}
\end{block}

\begin{block}{Prelude for C equivalent}<only@2>
\small
\begin{minted}{c}
typedef struct {
    void (*move)(const void*, const void*);
    /* other functions omitted */
} Type;

const Type Type_Int64 = /* omitted */;
\end{minted}
\end{block}

\note<1>{
Here's an example of the approach.
Polymorphic values are passed and returned via pointers, usually to stack memory.
For each type variable, a corresponding \texttt{type dictionary} is passed to the function.
Each type dictionary contains functions that manipulation pointers to values of a specific type.
\\ \\
For example, the \texttt{Type\_Int64} dictionary has functions that manipulate pointers to 64-bit integers.
}
\note<2>{
Here's the definition of \texttt{Type}.
\texttt{move} is the only function required to work with value types like integers.
If you have compound types and reference counting allocation you can add a \texttt{copy} and \texttt{drop} function.
}
\note<3>{
I find this approach interesting because it overlaps with both monomorphisation and uniform representation but can't be reduced to either.
\\ \\
It's similar to monomorphisation, in that monomorphic function arguments can be passed in registers or on the stack.
And it's similar to uniform representation in that polymorphic arguments are represented by pointers.
\\ \\
But \textit{unlike} monomorphisation, it permits separate compilation,
and \textit{unlike} uniform representation, it doesn't require all values to be heap-allocated pointers.
}
\end{frame}


\begin{frame}[fragile]
\frametitle{Compiling a polymorphic function (type passing)}

\begin{block}{A simple polymorphic function (Haskell-like language)}
\begin{minted}{haskell}
id :: forall a. a -> a
id x = x
\end{minted}
\end{block}

\begin{block}{A simple polymorphic function call (Haskell-like language)}
\begin{minted}{haskell}
let x = {-# SPECIALISE #-} id @Int64 42
\end{minted}
\end{block}

\begin{columns}
\begin{column}{0.48\textwidth}
\begin{block}{C equivalent (definition)}
\begin{minted}{c}
int64_t id_Int64(int64_t x) {
    return x;
}
\end{minted}
\end{block}
\end{column}

\begin{column}{0.48\textwidth}
\begin{block}{C equivalent (call)}
\begin{minted}{c}
int64_t x = id_Int64(42);
\end{minted}
\end{block}
\end{column}
\end{columns}

\note{
I also like this approach because it sets the stage for programmer-driven monomorphisation. 
\\ \\
If a polymorphic function is called repeatedly with a known type (e.g. in a loop),
then the programmer can choose to use a specialised version to avoid the overhead of passing arguments via the stack.
}
\end{frame}


\begin{frame}[fragile]
\frametitle{Pros and cons of type passing}

\begin{tabular}{l l}
Pros & Cons \\
\hline
\visible<2->{Generates fast monomorphic code} & \visible<6->{Polymorphism has runtime overhead} \\
\visible<3->{Reasonably compact code} & \\
\visible<4->{Allows separate compilation} & \\
\visible<5->{Supports advanced type system features} & \\
\end{tabular}

\note<2>{
The presence of polymorphism in the language doesn't impact monomorphic code.
Uniform representation mandates boxing by default in order to compile polymorphic code,
and if you don't use polymorphism you still have to pay the cost.*

* Unless you have a very advanced compiler like GHC.
}
\note<3>{
Use of polymorphic functions with many different types doesn't cause an explosion in code size.
However, the type dictionaries and extra function arguments take up space.
}
\note<4>{
Separate compilation works.
}
\note<5>{
The approach naturally generalises to higher-kinded types, existentials, GADTs, and maybe even dependent types.
I'm running out of steam so I'm not going to justify that claim as part of this presentation.
\\ \\
If people are interested we can sketch some of it if there's extra time left, or afterward at the pub.
}
\note<6>{
The main (theoretical) drawback is the runtime overhead of type dictionaries.

\begin{itemize}
\item These type dictionaries are extra function arguments
\item Calling a type dictionary requires a pointer dereference and indirect jump
\item More interesting cases require runtime construction of type dictionaries
\end{itemize}

I'm unsure of the magnitude of these performance impacts in real-world code.
It may be that it's just too slow in general, but it's also possible that slow cases are rare and easily fixed by programmer-driven specialisation.
\\ \\
I'm building a compiler using these techniques with the intention of benchmarking the type passing approach.
It's slow going because I have lots of other things to do, so if you're excited by these ideas then feel free to race ahead build the thing yourself so I can benchmark it.
}
\end{frame}


\begin{frame}[fragile]
\frametitle{Related work}

\begin{itemize}
\item How to make ad-hoc polymorphism less ad hoc (1989) - \url{https://doi.org/10.1145/75277.75283}
\item An ad hoc approach to the implementation of polymorphism (1991) - \url{https://doi.org/10.1145/117009.117017}
\item Dictionary passing for polytypic polymorphism (2001) - \url{https://www.cs.princeton.edu/techreports/2001/635.pdf}
\item A type-passing approach for the implementation of parametric methods in Java (2003) - \url{https://doi.org/10.1093/comjnl/46.3.263}
\item Implementing Swift Generics (2017) - \url{https://www.youtube.com/watch?v=ctS8FzqcRug}
\end{itemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{Bonus slides - existential types}

\begin{block}{A simple existential type (Haskell-like language)}
\begin{minted}{haskell}
exists a. a
\end{minted}
\end{block}

\begin{block}{C equivalent}
\begin{minted}{c}
typedef struct {
  const Type* type;
  const void* value;
} Exists_example;
\end{minted}
\end{block}

\note{
Logicians call parametric polymorphism ``universal quantification'', and existential quantification is its dual.
In previous slides the \texttt{forall} keyword gave rise to extra function arguments for type dictionaries.
On the other hand, \texttt{exists} corresponds to pairs/products/structures that have a field for each type variable.
\\ \\
\texttt{exists a. a} is not very useful; all you can do with it is pass it around.
It's a useful minimal example nonetheless.
\\ \\
\texttt{exists a. a} is a type dictionary paired with a pointer to a value that the type dictionary can manipulate.
\\ \\
Notice that it contains a freestanding pointer.
I claim that to compile unrestricted existential quantification you need a memory management system.
I've started my experiments with automatic reference counting.
}
\end{frame}


\begin{frame}[fragile]
\frametitle{Bonus slides - higher-kinded types}

\begin{block}{A simple function with higher-kinded polymorphism (Haskell-like language)}
\begin{minted}{haskell}
id' :: forall (f :: Type -> Type) (a :: Type). f a -> f a
id' x = x
\end{minted}
\end{block}

\begin{block}{C equivalent}
\begin{minted}{c}
void id_prime(
    void* result,
    const TypeToType* f,
    const Type* a,
    const void* x
) {
    const Type* f_a = f->apply(a);
    f_a->move(result, x);
}
\end{minted}
\end{block}

\note{
Here's a silly identity function that only works on `container-shaped' types.
When compiled, it gets an extra argument for each of the two type variables.
\\ \\
\texttt{f} is not a type dictionary, though - it's a type dictionary \textit{constructor}.
\\ \\
Notice that the constructed dictionary \texttt{f\_a} is behind a pointer.
I think that higher-kinded types in the type passing approach also requires automatic memory management.
For example, if \texttt{f a} was packed into the type component of \texttt{exists a. a} from earlier.
\\ \\
This kind (ha ha) of complication is why I'd like to benchmark real-world code compiled with this approach.
Part of the reason I'm thinking about any of this is to reduce allocations,
and it would potentially be a shame to just have moved them around.
If higher-kinded types in this style were too slow or used too much memory to be practical,
it would be better to not support them in the first place!
I'm moderately optimistic, though.
}
\end{frame}


\end{document}
